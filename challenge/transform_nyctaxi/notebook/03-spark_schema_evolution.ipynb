{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import when, count, col, lit, year, month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "  .builder\n",
    "  .appName(\"nyctlc-dev\")\n",
    "  .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = \"s3://your_bucket_name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Green taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_path = f'{s3_bucket}/raw/nyc-tlc/green'\n",
    "green_2013 = f'{green_path}/green_tripdata_2013*.csv.gz'\n",
    "green_2014 = f'{green_path}/green_tripdata_2014*.csv.gz'\n",
    "green_2015 = f'{green_path}/green_tripdata_2015*.csv.gz'\n",
    "green_2016 = f'{green_path}/green_tripdata_2016*.csv.gz'\n",
    "green_2017 = f'{green_path}/green_tripdata_2017*.csv.gz'\n",
    "green_2018 = f'{green_path}/green_tripdata_2018*.csv.gz'\n",
    "green_2019 = f'{green_path}/green_tripdata_2019*.csv.gz'\n",
    "green_2020 = f'{green_path}/green_tripdata_2020*.csv.gz'\n",
    "green_output_path = f'{s3_bucket}/staging/nyc-tlc/green'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore 2013-2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start exploring the 2013-2014 data by instructing Spark to infer schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_raw = spark.read.csv(\n",
    "    [green_2013, green_2014],\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    timestampFormat='yyyy-MM-dd HH:mm:ss',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross check with the S3 select result\n",
    "df1_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_raw.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting null values\n",
    "df1_raw.select([count(when(col(c).isNull(), c)).alias(c) for c in df1_raw.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load\n",
    "### 2013-2014\n",
    "Data looks good. Now customize the schema and load again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes:\n",
    "# Upper case > lower case\n",
    "# Ehail_fee - double\n",
    "\n",
    "\n",
    "df1_schema = StructType([\n",
    "    StructField('VendorID',IntegerType(),True),\n",
    "    StructField('lpep_pickup_datetime',TimestampType(),True),\n",
    "    StructField('lpep_dropoff_datetime',TimestampType(),True),\n",
    "    StructField('store_and_fwd_flag',StringType(),True),\n",
    "    StructField('RatecodeID',IntegerType(),True),\n",
    "    StructField('Pickup_longitude',DoubleType(),True),\n",
    "    StructField('Pickup_latitude',DoubleType(),True),\n",
    "    StructField('Dropoff_longitude',DoubleType(),True),\n",
    "    StructField('Dropoff_latitude',DoubleType(),True),\n",
    "    StructField('passenger_count',IntegerType(),True),\n",
    "    StructField('trip_distance',DoubleType(),True),\n",
    "    StructField('fare_amount',DoubleType(),True),\n",
    "    StructField('extra',DoubleType(),True),\n",
    "    StructField('mta_tax',DoubleType(),True),\n",
    "    StructField('tip_amount',DoubleType(),True),\n",
    "    StructField('tolls_amount',DoubleType(),True),\n",
    "    StructField('ehail_fee',DoubleType(),True),\n",
    "    StructField('total_amount',DoubleType(),True),\n",
    "    StructField('payment_type',IntegerType(),True),\n",
    "    StructField('trip_type' ,IntegerType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_raw = spark.read.csv(\n",
    "    [green_2013, green_2014],\n",
    "    schema=df1_schema,\n",
    "    header=True,\n",
    "    timestampFormat='yyyy-MM-dd HH:mm:ss',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-check with the one with schema inferred \n",
    "df1_raw.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more columns to make the schema the same across all years\n",
    "# Add: improvement_surcharge, PULocationID, DOLocationID, congestion_surcharge\n",
    "\n",
    "df_2013_2014 = (df1_raw.withColumn('improvement_surcharge', lit(None).astype(DoubleType()))\n",
    "                .withColumn('PULocationID', lit(None).astype(IntegerType()))\n",
    "                .withColumn('DOLocationID', lit(None).astype(IntegerType()))\n",
    "                .withColumn('congestion_surcharge', lit(None).astype(DoubleType()))\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013_2014.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2015-2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes from 2013-2014:\n",
    "# Add: improvement_surcharge\n",
    "\n",
    "\n",
    "df2_schema = StructType([\n",
    "    StructField('VendorID',IntegerType(),True),\n",
    "    StructField('lpep_pickup_datetime',TimestampType(),True),\n",
    "    StructField('lpep_dropoff_datetime',TimestampType(),True),\n",
    "    StructField('store_and_fwd_flag',StringType(),True),\n",
    "    StructField('RatecodeID',IntegerType(),True),\n",
    "    StructField('Pickup_longitude',DoubleType(),True),\n",
    "    StructField('Pickup_latitude',DoubleType(),True),\n",
    "    StructField('Dropoff_longitude',DoubleType(),True),\n",
    "    StructField('Dropoff_latitude',DoubleType(),True),\n",
    "    StructField('passenger_count',IntegerType(),True),\n",
    "    StructField('trip_distance',DoubleType(),True),\n",
    "    StructField('fare_amount',DoubleType(),True),\n",
    "    StructField('extra',DoubleType(),True),\n",
    "    StructField('mta_tax',DoubleType(),True),\n",
    "    StructField('tip_amount',DoubleType(),True),\n",
    "    StructField('tolls_amount',DoubleType(),True),\n",
    "    StructField('ehail_fee',DoubleType(),True),\n",
    "    StructField('improvement_surcharge',DoubleType(),True),\n",
    "    StructField('total_amount',DoubleType(),True),\n",
    "    StructField('payment_type',IntegerType(),True),\n",
    "    StructField('trip_type' ,IntegerType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_raw = spark.read.csv(\n",
    "    [green_2015, green_2016],\n",
    "    schema=df2_schema,\n",
    "    header=True,\n",
    "    timestampFormat='yyyy-MM-dd HH:mm:ss',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross check with the S3 select result\n",
    "df2_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more columns to make the schema the same across all years\n",
    "# Add: improvement_surcharge, PULocationID, DOLocationID, congestion_surcharge\n",
    "\n",
    "df_2015_2016 = (df2_raw.withColumn('PULocationID', lit(None).astype(IntegerType()))\n",
    "                .withColumn('DOLocationID', lit(None).astype(IntegerType()))\n",
    "                .withColumn('congestion_surcharge', lit(None).astype(DoubleType()))\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2015_2016.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2017-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes from 2015-2016:\n",
    "# Remove: Pickup_longitude, Pickup_latitude, Dropoff_longitude, Dropoff_latitude\n",
    "# Add: PULocationID, DOLocationID\n",
    "\n",
    "\n",
    "df3_schema = StructType([\n",
    "    StructField('VendorID',IntegerType(),True),\n",
    "    StructField('lpep_pickup_datetime',TimestampType(),True),\n",
    "    StructField('lpep_dropoff_datetime',TimestampType(),True),\n",
    "    StructField('store_and_fwd_flag',StringType(),True),\n",
    "    StructField('RatecodeID',IntegerType(),True),\n",
    "    StructField('PULocationID',IntegerType(),True),\n",
    "    StructField('DOLocationID',IntegerType(),True),\n",
    "    StructField('passenger_count',IntegerType(),True),\n",
    "    StructField('trip_distance',DoubleType(),True),\n",
    "    StructField('fare_amount',DoubleType(),True),\n",
    "    StructField('extra',DoubleType(),True),\n",
    "    StructField('mta_tax',DoubleType(),True),\n",
    "    StructField('tip_amount',DoubleType(),True),\n",
    "    StructField('tolls_amount',DoubleType(),True),\n",
    "    StructField('ehail_fee',DoubleType(),True),\n",
    "    StructField('improvement_surcharge',DoubleType(),True),\n",
    "    StructField('total_amount',DoubleType(),True),\n",
    "    StructField('payment_type',IntegerType(),True),\n",
    "    StructField('trip_type' ,IntegerType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_raw = spark.read.csv(\n",
    "    [green_2017, green_2018],\n",
    "    schema=df3_schema,\n",
    "    header=True,\n",
    "    timestampFormat='yyyy-MM-dd HH:mm:ss',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross check with the S3 select result\n",
    "df3_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more columns to make the schema the same across all years\n",
    "# Add: Pickup_longitude, Pickup_latitude, Dropoff_longitude, Dropoff_latitude, congestion_surcharge\n",
    "\n",
    "df_2017_2018 = (df3_raw.withColumn('Pickup_longitude', lit(None).astype(DoubleType()))\n",
    "                .withColumn('Pickup_latitude', lit(None).astype(DoubleType()))\n",
    "                .withColumn('Dropoff_longitude', lit(None).astype(DoubleType()))\n",
    "                .withColumn('Dropoff_latitude', lit(None).astype(DoubleType()))\n",
    "                .withColumn('congestion_surcharge', lit(None).astype(DoubleType()))\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017_2018.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2019-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes from 2017-2018:\n",
    "# Add: congestion_surcharge\n",
    "\n",
    "\n",
    "df4_schema = StructType([\n",
    "    StructField('VendorID',IntegerType(),True),\n",
    "    StructField('lpep_pickup_datetime',TimestampType(),True),\n",
    "    StructField('lpep_dropoff_datetime',TimestampType(),True),\n",
    "    StructField('store_and_fwd_flag',StringType(),True),\n",
    "    StructField('RatecodeID',IntegerType(),True),\n",
    "    StructField('PULocationID',IntegerType(),True),\n",
    "    StructField('DOLocationID',IntegerType(),True),\n",
    "    StructField('passenger_count',IntegerType(),True),\n",
    "    StructField('trip_distance',DoubleType(),True),\n",
    "    StructField('fare_amount',DoubleType(),True),\n",
    "    StructField('extra',DoubleType(),True),\n",
    "    StructField('mta_tax',DoubleType(),True),\n",
    "    StructField('tip_amount',DoubleType(),True),\n",
    "    StructField('tolls_amount',DoubleType(),True),\n",
    "    StructField('ehail_fee',DoubleType(),True),\n",
    "    StructField('improvement_surcharge',DoubleType(),True),\n",
    "    StructField('total_amount',DoubleType(),True),\n",
    "    StructField('payment_type',IntegerType(),True),\n",
    "    StructField('trip_type' ,IntegerType(),True),\n",
    "    StructField('congestion_surcharge',DoubleType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_raw = spark.read.csv(\n",
    "    [green_2019, green_2020],\n",
    "    schema=df4_schema,\n",
    "    header=True,\n",
    "    timestampFormat='yyyy-MM-dd HH:mm:ss',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross check with the S3 select result\n",
    "df4_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more columns to make the schema the same across all years\n",
    "# Add: Pickup_longitude, Pickup_latitude, Dropoff_longitude, Dropoff_latitude\n",
    "\n",
    "df_2019_2020 = (df4_raw.withColumn('Pickup_longitude', lit(None).astype(DoubleType()))\n",
    "                .withColumn('Pickup_latitude', lit(None).astype(DoubleType()))\n",
    "                .withColumn('Dropoff_longitude', lit(None).astype(DoubleType()))\n",
    "                .withColumn('Dropoff_latitude', lit(None).astype(DoubleType()))\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019_2020.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = (df_2013_2014\n",
    "            .unionByName(df_2015_2016)\n",
    "            .unionByName(df_2017_2018)\n",
    "            .unionByName(df_2019_2020)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Sink the df to staging partitioned by pickup year and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_final = (df_green\n",
    "                  .withColumn(\"year\", year('lpep_pickup_datetime'))\n",
    "                  .withColumn(\"month\", month('lpep_pickup_datetime'))\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = ['year', 'month']\n",
    "\n",
    "(df_green_final\n",
    " .repartition(col(partitions[0]), col(partitions[1]))\n",
    " .write.mode(\"OVERWRITE\")\n",
    " .option(\"maxRecordsPerFile\", 1000000)\n",
    " .partitionBy(partitions)\n",
    " .parquet(green_output_path, compression=\"gzip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FHV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_path = f'{s3_bucket}/raw/nyc-tlc/fhv'\n",
    "fhv_2015 = f'{fhv_path}/fhv_tripdata_2015*.csv.gz'\n",
    "fhv_2016 = f'{fhv_path}/fhv_tripdata_2016*.csv.gz'\n",
    "fhv_2017 = f'{fhv_path}/fhv_tripdata_2017*.csv.gz'\n",
    "fhv_2018 = f'{fhv_path}/fhv_tripdata_2018*.csv.gz'\n",
    "fhv_2019 = f'{fhv_path}/fhv_tripdata_2019*.csv.gz'\n",
    "fhv_2020 = f'{fhv_path}/fhv_tripdata_2020*.csv.gz'\n",
    "fhv_output_path = f'{s3_bucket}/staging/nyc-tlc/fhv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore 2020\n",
    "\n",
    "Infer the schema and make manual adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_raw = spark.read.csv(\n",
    "    [fhv_2020],\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    timestampFormat='yyyy-MM-dd HH:mm:ss',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_raw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_2020_schema = StructType([\n",
    "    StructField('dispatching_base_num',StringType(),True),\n",
    "    StructField('pickup_datetime',TimestampType(),True),\n",
    "    StructField('dropoff_datetime',TimestampType(),True),\n",
    "    StructField('PULocationID',IntegerType(),True),\n",
    "    StructField('DOLocationID',IntegerType(),True),\n",
    "    StructField('SR_Flag',StringType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore 2015: Does Pickup_date includes time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_raw = spark.read.csv(\n",
    "    [fhv_2015],\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    timestampFormat='yyyy-MM-dd HH:mm:ss',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_raw.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore 2018: Duplicated dispatching_num?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_2018_schema = StructType([\n",
    "    StructField('pickup_datetime',TimestampType(),True),\n",
    "    StructField('dropoff_datetime',TimestampType(),True),\n",
    "    StructField('PULocationID',IntegerType(),True),\n",
    "    StructField('DOLocationID',IntegerType(),True),\n",
    "    StructField('dispatching_base_number',StringType(),True),\n",
    "    StructField('dispatching_base_num',StringType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_raw = spark.read.csv(\n",
    "    [fhv_2018],\n",
    "    schema=fhv_2018_schema,\n",
    "    header=True,\n",
    "    timestampFormat='yyyy-MM-dd HH:mm:ss',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_raw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the column all null?\n",
    "df2_raw.select(count(when(col('dispatching_base_number').isNull(), 1))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the column all null?\n",
    "df2_raw.select(count(when(col('dispatching_base_num').isNull(), 1))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are those not null values?\n",
    "df2_raw.filter(df2_raw['dispatching_base_number'].isNotNull()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the column only contain 1 and null? Yes\n",
    "df2_raw.select('dispatching_base_number').summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2015-2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_2015_schema = StructType([\n",
    "    StructField('dispatching_base_num',StringType(),True),\n",
    "    StructField('pickup_datetime',TimestampType(),True),\n",
    "    StructField('PULocationID',IntegerType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_2015_2016_raw = spark.read.csv(\n",
    "    [fhv_2015, fhv_2016],\n",
    "    schema=fhv_2015_schema,\n",
    "    header=True,\n",
    "    timestampFormat='yyyy-MM-dd HH:mm:ss',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more columns to make the schema the same across all years\n",
    "# Add: dropoff_datetime, DOlocationID, SR_Flag\n",
    "\n",
    "fhv_2015_2016 = (fhv_2015_2016_raw.withColumn('dropoff_datetime', lit(None).astype(TimestampType()))\n",
    "                 .withColumn('DOlocationID', lit(None).astype(IntegerType()))\n",
    "                 .withColumn('SR_Flag', lit(None).astype(StringType()))\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_2017_schema = StructType([\n",
    "    StructField('dispatching_base_num',StringType(),True),\n",
    "    StructField('pickup_datetime',TimestampType(),True),\n",
    "    StructField('dropoff_datetime',TimestampType(),True),\n",
    "    StructField('PULocationID',IntegerType(),True),\n",
    "    StructField('DOLocationID',IntegerType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_2017_raw = spark.read.csv(\n",
    "    [fhv_2017],\n",
    "    schema=fhv_2017_schema,\n",
    "    header=True,\n",
    "    timestampFormat='yyyy-MM-dd HH:mm:ss',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more columns to make the schema the same across all years\n",
    "# Add: SR_Flag\n",
    "\n",
    "fhv_2017 = fhv_2017_raw.withColumn('SR_Flag', lit(None).astype(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_2018_schema = StructType([\n",
    "    StructField('pickup_datetime',TimestampType(),True),\n",
    "    StructField('dropoff_datetime',TimestampType(),True),\n",
    "    StructField('PULocationID',IntegerType(),True),\n",
    "    StructField('DOLocationID',IntegerType(),True),\n",
    "    StructField('dispatching_base_number',StringType(),True),\n",
    "    StructField('dispatching_base_num',StringType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_2018_raw = spark.read.csv(\n",
    "    [fhv_2018],\n",
    "    schema=fhv_2018_schema,\n",
    "    header=True,\n",
    "    timestampFormat='yyyy-MM-dd HH:mm:ss',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more columns to make the schema the same across all years\n",
    "# Add: SR_Flag\n",
    "# Drop: dispatching_base_number\n",
    "\n",
    "fhv_2018 = (fhv_2018_raw.withColumn('SR_Flag', lit(None).astype(StringType()))\n",
    "            .drop('dispatching_base_number')\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2019-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_2020_schema = StructType([\n",
    "    StructField('dispatching_base_num',StringType(),True),\n",
    "    StructField('pickup_datetime',TimestampType(),True),\n",
    "    StructField('dropoff_datetime',TimestampType(),True),\n",
    "    StructField('PULocationID',IntegerType(),True),\n",
    "    StructField('DOLocationID',IntegerType(),True),\n",
    "    StructField('SR_Flag',StringType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_2019_2020_raw = spark.read.csv(\n",
    "    [fhv_2019, fhv_2020],\n",
    "    schema=fhv_2020_schema,\n",
    "    header=True,\n",
    "    timestampFormat='yyyy-MM-dd HH:mm:ss',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv = (fhv_2015_2016\n",
    "            .unionByName(fhv_2017)\n",
    "            .unionByName(fhv_2018)\n",
    "            .unionByName(fhv_2019_2020_raw)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Sink the df to staging partitioned by pickup year and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv_final = (df_fhv\n",
    "                  .withColumn(\"year\", year('pickup_datetime'))\n",
    "                  .withColumn(\"month\", month('pickup_datetime'))\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = ['year', 'month']\n",
    "\n",
    "(df_fhv_final\n",
    " .repartition(col(partitions[0]), col(partitions[1]))\n",
    " .write.mode(\"OVERWRITE\")\n",
    " .option(\"maxRecordsPerFile\", 1000000)\n",
    " .partitionBy(partitions)\n",
    " .parquet(fhv_output_path, compression=\"gzip\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
